# reproduce_with_seeds.py
"""
REPRODUCIR 0.6440 PROBANDO MÃšLTIPLES SEEDS

El problema probablemente es que el seed no estÃ¡ completamente fijo
"""

import torch
import torch.nn as nn
from torch.utils.data import DataLoader
from sklearn.metrics import roc_auc_score
from sklearn.model_selection import train_test_split
from tqdm import tqdm
import json
import numpy as np
import random
import time
from pathlib import Path
import pandas as pd
import os

from mejorar_resnet3d import *

def set_all_seeds(seed):
    """Fijar TODAS las semillas posibles para mÃ¡ximo determinismo"""
    
    # Python random
    random.seed(seed)
    
    # NumPy
    np.random.seed(seed)
    
    # PyTorch
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    
    # PyTorch deterministic behavior
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False
    
    # Python hash seed
    os.environ['PYTHONHASHSEED'] = str(seed)
    
    print(f"âœ… Todas las semillas fijadas a: {seed}")

def quick_train_test(seed, target_auc=0.6440, max_epochs=25):
    """Entrenamiento rÃ¡pido para probar un seed especÃ­fico"""
    
    print(f"\nğŸ² PROBANDO SEED: {seed}")
    print("=" * 30)
    
    # Fijar todas las semillas
    set_all_seeds(seed)
    
    # ConfiguraciÃ³n exacta ganadora
    FastConfig.TARGET_SHAPE = (80, 80, 80)
    FastConfig.INPUT_CHANNELS = 3
    FastConfig.MODEL_TYPE = 'resnet14t'
    FastConfig.BATCH_SIZE = 12
    FastConfig.EPOCHS = max_epochs  # Menos Ã©pocas para probar rÃ¡pido
    FastConfig.LEARNING_RATE = 8e-4
    FastConfig.WEIGHT_DECAY = 1e-4
    FastConfig.PATIENCE = 8  # Menos paciencia
    FastConfig.SEED = seed
    
    try:
        # Cargar datos exactos
        OFFICIAL_SPLITS_CSV = Path(r"C:\Users\usuario\Documents\Mama_Mia\datos\train_test_splits.csv")
        df = pd.read_csv(OFFICIAL_SPLITS_CSV)
        train_ids_csv = df['train_split'].dropna().unique().tolist()
        test_ids_csv = df['test_split'].dropna().unique().tolist()
        
        # Labels
        with open(FastConfig.LABELS_FILE, 'r') as f:
            pcr_list = json.load(f)
        
        pcr_data = {item['patient_id']: item for item in pcr_list}
        
        # Filtrar train vÃ¡lidos
        train_ids, train_labels = [], []
        for pid in train_ids_csv:
            pid = str(pid)
            if pid in pcr_data and 'pcr' in pcr_data[pid]:
                if pcr_data[pid]['pcr'] in ["0", "1"]:
                    tensor_path = FastConfig.DATA_DIR / pid / f"{pid}_tensor_3ch.nii.gz"
                    if tensor_path.exists():
                        train_ids.append(pid)
                        train_labels.append(int(pcr_data[pid]['pcr']))
        
        # Train-val split CON EL SEED ESPECÃFICO
        train_train_ids, val_ids, train_train_labels, val_labels = train_test_split(
            train_ids, train_labels, test_size=0.2, random_state=seed, stratify=train_labels
        )
        
        print(f"ğŸ“Š Split con seed {seed}:")
        print(f"  Train: {len(train_train_ids)}, Val: {len(val_ids)}")
        print(f"  Train pCR: {np.mean(train_train_labels):.1%}, Val pCR: {np.mean(val_labels):.1%}")
        
        # Datasets
        train_dataset = FastDataset(
            FastConfig.DATA_DIR, train_train_ids, train_train_labels,
            augment=True, target_shape=FastConfig.TARGET_SHAPE,
            input_channels=FastConfig.INPUT_CHANNELS
        )
        val_dataset = FastDataset(
            FastConfig.DATA_DIR, val_ids, val_labels,
            augment=False, target_shape=FastConfig.TARGET_SHAPE,
            input_channels=FastConfig.INPUT_CHANNELS
        )
        
        # DataLoaders
        train_loader = DataLoader(
            train_dataset, batch_size=FastConfig.BATCH_SIZE,
            shuffle=True, num_workers=2, pin_memory=True
        )
        val_loader = DataLoader(
            val_dataset, batch_size=FastConfig.BATCH_SIZE,
            shuffle=False, num_workers=2, pin_memory=True
        )
        
        # Modelo
        trainer = FastTrainer()
        model = trainer.create_model()
        
        # Training rÃ¡pido
        device = trainer.device
        pos_weight = torch.tensor([2.39]).to(device)
        criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)
        
        optimizer = torch.optim.AdamW(
            model.parameters(),
            lr=FastConfig.LEARNING_RATE,
            weight_decay=FastConfig.WEIGHT_DECAY
        )
        
        scheduler = trainer.create_scheduler(optimizer, len(train_loader))
        
        best_auc = 0
        patience_counter = 0
        
        for epoch in range(FastConfig.EPOCHS):
            # Training
            model.train()
            train_losses = []
            
            for batch in train_loader:
                tensors = batch['tensor'].to(device, non_blocking=True)
                targets = batch['target'].to(device, non_blocking=True).float()
                
                optimizer.zero_grad()
                
                if trainer.use_amp:
                    with torch.autocast(device_type='cuda', dtype=trainer.dtype):
                        outputs = model(tensors).squeeze()
                        loss = criterion(outputs, targets)
                    
                    trainer.scaler.scale(loss).backward()
                    
                    if FastConfig.USE_GRADIENT_CLIPPING:
                        trainer.scaler.unscale_(optimizer)
                        torch.nn.utils.clip_grad_norm_(model.parameters(), FastConfig.CLIP_VALUE)
                    
                    trainer.scaler.step(optimizer)
                    trainer.scaler.update()
                else:
                    outputs = model(tensors).squeeze()
                    loss = criterion(outputs, targets)
                    loss.backward()
                    
                    if FastConfig.USE_GRADIENT_CLIPPING:
                        torch.nn.utils.clip_grad_norm_(model.parameters(), FastConfig.CLIP_VALUE)
                    
                    optimizer.step()
                
                if scheduler:
                    scheduler.step()
                
                train_losses.append(loss.item())
            
            # Validation
            model.eval()
            val_preds, val_targets = [], []
            
            with torch.no_grad():
                for batch in val_loader:
                    tensors = batch['tensor'].to(device, non_blocking=True)
                    targets = batch['target'].to(device, non_blocking=True).float()
                    
                    if trainer.use_amp:
                        with torch.autocast(device_type='cuda', dtype=trainer.dtype):
                            outputs = model(tensors).squeeze()
                    else:
                        outputs = model(tensors).squeeze()
                    
                    probs = torch.sigmoid(outputs)
                    val_preds.extend(safe_to_numpy(probs))
                    val_targets.extend(safe_to_numpy(targets))
            
            val_auc = roc_auc_score(val_targets, val_preds)
            
            if val_auc > best_auc:
                best_auc = val_auc
                patience_counter = 0
                
                # Guardar si estÃ¡ cerca del objetivo
                if val_auc > target_auc - 0.02:
                    model_name = f'seed_{seed}_auc{val_auc:.4f}.pth'
                    torch.save(model.state_dict(), model_name)
            else:
                patience_counter += 1
                if patience_counter >= FastConfig.PATIENCE:
                    break
        
        print(f"  ğŸ¯ Best Val AUC: {best_auc:.4f}")
        
        # Evaluar en test si estÃ¡ cerca
        if best_auc > target_auc - 0.02:
            print(f"  ğŸ§ª Evaluando en test...")
            
            # Test data
            test_ids, test_labels = [], []
            for pid in test_ids_csv:
                pid = str(pid)
                if pid in pcr_data and 'pcr' in pcr_data[pid]:
                    if pcr_data[pid]['pcr'] in ["0", "1"]:
                        tensor_path = FastConfig.DATA_DIR / pid / f"{pid}_tensor_3ch.nii.gz"
                        if tensor_path.exists():
                            test_ids.append(pid)
                            test_labels.append(int(pcr_data[pid]['pcr']))
            
            test_dataset = FastDataset(
                FastConfig.DATA_DIR, test_ids, test_labels,
                augment=False, target_shape=FastConfig.TARGET_SHAPE,
                input_channels=FastConfig.INPUT_CHANNELS
            )
            
            test_loader = DataLoader(
                test_dataset, batch_size=FastConfig.BATCH_SIZE,
                shuffle=False, num_workers=2, pin_memory=True
            )
            
            test_preds, test_targets = [], []
            
            with torch.no_grad():
                for batch in test_loader:
                    tensors = batch['tensor'].to(device, non_blocking=True)
                    targets = batch['target'].to(device, non_blocking=True).float()
                    
                    if trainer.use_amp:
                        with torch.autocast(device_type='cuda', dtype=trainer.dtype):
                            outputs = model(tensors).squeeze()
                    else:
                        outputs = model(tensors).squeeze()
                    
                    probs = torch.sigmoid(outputs)
                    test_preds.extend(safe_to_numpy(probs))
                    test_targets.extend(safe_to_numpy(targets))
            
            test_auc = roc_auc_score(test_targets, test_preds)
            print(f"  ğŸ² Test AUC: {test_auc:.4f}")
            
            return best_auc, test_auc
        
        return best_auc, None
        
    except Exception as e:
        print(f"  âŒ Error: {e}")
        return 0.0, None

def find_winning_seed():
    """Probar mÃºltiples seeds para encontrar el que reproduce 0.6440"""
    
    print("ğŸ² BÃšSQUEDA DEL SEED GANADOR")
    print("ğŸ¯ Objetivo: Reproducir Val AUC â‰ˆ 0.6440")
    print("=" * 60)
    
    # Seeds a probar (incluir 42 y variaciones)
    seeds_to_try = [
        42,    # Original
        0,     # PyTorch default
        123,   # ComÃºn
        456,   # 
        789,   #
        2023,  # AÃ±o
        2024,  # 
        2025,  #
        1337,  # Leet
        9999,  # 
        7777,  # Lucky
        1234,  # Simple
        5678,  #
        2468,  # Even
        1357,  # Odd
    ]
    
    results = []
    target_auc = 0.6440
    
    for i, seed in enumerate(seeds_to_try):
        print(f"\nğŸ”„ Progreso: {i+1}/{len(seeds_to_try)}")
        
        start_time = time.time()
        val_auc, test_auc = quick_train_test(seed, target_auc)
        elapsed = time.time() - start_time
        
        result = {
            'seed': seed,
            'val_auc': float(val_auc) if val_auc else 0.0,
            'test_auc': float(test_auc) if test_auc else None,
            'time_minutes': float(elapsed / 60),
            'close_to_target': bool(abs(val_auc - target_auc) < 0.02 if val_auc > 0 else False)
        }
        
        results.append(result)
        
        # Guardar progreso
        with open('seed_search_progress.json', 'w') as f:
            json.dump(results, f, indent=2)
        
        if val_auc > target_auc - 0.01:
            print(f"ğŸ‰ Â¡SEED PROMETEDOR ENCONTRADO! {seed}: Val {val_auc:.4f}")
            if test_auc and test_auc > 0.6300:
                print(f"ğŸ† Â¡Y TEST AUC EXCELENTE! {test_auc:.4f}")
                break
    
    # AnÃ¡lisis final
    successful_results = [r for r in results if r['val_auc'] > target_auc - 0.03]
    
    print(f"\nğŸ¯ RESULTADOS DE BÃšSQUEDA:")
    print("=" * 50)
    
    if successful_results:
        # Ordenar por val_auc
        successful_results.sort(key=lambda x: x['val_auc'], reverse=True)
        
        print(f"âœ… Seeds prometedores encontrados: {len(successful_results)}")
        
        for result in successful_results[:5]:  # Top 5
            test_str = f", Test: {result['test_auc']:.4f}" if result['test_auc'] else ""
            print(f"  ğŸ² Seed {result['seed']}: Val {result['val_auc']:.4f}{test_str}")
        
        best = successful_results[0]
        print(f"\nğŸ† MEJOR SEED: {best['seed']}")
        print(f"ğŸ¯ Val AUC: {best['val_auc']:.4f}")
        
        if best['test_auc']:
            print(f"ğŸ² Test AUC: {best['test_auc']:.4f}")
            
            if best['test_auc'] > 0.6200:
                print(f"ğŸ‰ Â¡EXCELENTE! ReproducciÃ³n exitosa")
            else:
                print(f"âš ï¸ Test AUC menor a esperado")
    
    else:
        print(f"âŒ No se encontrÃ³ ningÃºn seed que reproduzca ~0.6440")
        print(f"ğŸ’¡ Probar con:")
        print(f"   - MÃ¡s seeds")
        print(f"   - MÃ¡s Ã©pocas")
        print(f"   - ConfiguraciÃ³n ligeramente diferente")
    
    # Guardar resultados finales
    with open('seed_search_final.json', 'w') as f:
        json.dump(results, f, indent=2)
    
    print(f"\nğŸ’¾ Resultados guardados: seed_search_final.json")
    
    return results

def main():
    print("ğŸ² REPRODUCCIÃ“N CON BÃšSQUEDA DE SEEDS")
    print("ğŸ¯ HipÃ³tesis: El problema es la semilla aleatoria")
    print("=" * 60)
    
    results = find_winning_seed()
    
    best_results = [r for r in results if r['val_auc'] > 0.6200]
    
    if best_results:
        best = max(best_results, key=lambda x: x['val_auc'])
        print(f"\nâœ… BÃšSQUEDA EXITOSA:")
        print(f"ğŸ† Seed ganador: {best['seed']}")
        print(f"ğŸ¯ Val AUC: {best['val_auc']:.4f}")
        if best['test_auc']:
            print(f"ğŸ² Test AUC: {best['test_auc']:.4f}")
    else:
        print(f"\nâŒ BÃºsqueda no exitosa")
        print(f"ğŸ’¡ El problema puede ser mÃ¡s profundo que solo seeds")

if __name__ == "__main__":
    main()