# C√ìDIGO COMPLETO: TU H√çBRIDO + AN√ÅLISIS DE EQUIDAD AUTOM√ÅTICO
# GUARDA COMO: hybrid_complete_with_equity.py
# NO NECESITAS A√ëADIR NADA - SOLO EJECUTA

"""
H√çBRIDO COMPLETO CON AN√ÅLISIS DE EQUIDAD AUTOM√ÅTICO
===================================================

üîß INCLUYE: Todo tu c√≥digo original corregido
‚öñÔ∏è A√ëADE: An√°lisis de equidad multic√©ntrico autom√°tico  
üéØ GENERA: Figuras y reportes listos para matr√≠cula de honor
üöÄ USO: python hybrid_complete_with_equity.py
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader
import nibabel as nib
import numpy as np
import pandas as pd
from pathlib import Path
import json
from sklearn.model_selection import StratifiedKFold, train_test_split
from sklearn.metrics import roc_auc_score, accuracy_score, confusion_matrix, roc_curve
from sklearn.utils.class_weight import compute_class_weight
from sklearn.utils import resample
import scipy.stats as stats
import warnings
import random
from tqdm import tqdm
import time
import copy
import pickle
import matplotlib.pyplot as plt
import seaborn as sns

warnings.filterwarnings('ignore')

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f"üöÄ Using device: {device}")

# Configuraci√≥n para figuras de matr√≠cula de honor
plt.style.use('default')  # Cambiado para evitar problemas de versiones
plt.rcParams.update({
    'figure.figsize': (16, 10),
    'font.size': 12,
    'font.family': 'serif',
    'figure.dpi': 300,
    'savefig.dpi': 300,
    'savefig.bbox': 'tight',
    'axes.grid': True,
    'grid.alpha': 0.3,
    'axes.spines.top': False,
    'axes.spines.right': False
})

# =============================================================================
# CONFIGURACI√ìN DE SEMILLAS GANADORAS
# =============================================================================

def set_deterministic_seed(seed):
    """Configuraci√≥n determin√≠stica completa"""
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False

# =============================================================================
# EMA SIMPLE Y FUNCIONAL
# =============================================================================

class SimpleEMA:
    """EMA simple que funciona siempre"""
    def __init__(self, model, decay=0.999):
        self.model = model
        self.decay = decay
        self.shadow = {}
        self.backup = {}
        
        for name, param in model.named_parameters():
            if param.requires_grad:
                self.shadow[name] = param.data.clone()
    
    def update(self):
        for name, param in self.model.named_parameters():
            if param.requires_grad and name in self.shadow:
                self.shadow[name] = self.decay * self.shadow[name] + (1 - self.decay) * param.data
    
    def apply_shadow(self):
        for name, param in self.model.named_parameters():
            if param.requires_grad and name in self.shadow:
                self.backup[name] = param.data.clone()
                param.data = self.shadow[name]
    
    def restore(self):
        for name, param in self.model.named_parameters():
            if param.requires_grad and name in self.backup:
                param.data = self.backup[name]
        self.backup = {}

# =============================================================================
# MODELOS SIMPLES Y R√ÅPIDOS
# =============================================================================

class FastResNet3D(nn.Module):
    """ResNet 3D simple y r√°pido"""
    
    def __init__(self, in_channels=3, num_classes=2, model_size='small'):
        super().__init__()
        
        if model_size == 'tiny':
            channels = [16, 32, 64, 128]
            blocks = [1, 1, 1, 1]
        elif model_size == 'small':
            channels = [32, 64, 128, 256]
            blocks = [2, 2, 2, 2]
        else:  # medium
            channels = [64, 128, 256, 512]
            blocks = [2, 2, 3, 2]
        
        self.conv1 = nn.Conv3d(in_channels, channels[0], 7, stride=2, padding=3, bias=False)
        self.bn1 = nn.BatchNorm3d(channels[0])
        self.relu = nn.ReLU(inplace=True)
        self.maxpool = nn.MaxPool3d(3, stride=2, padding=1)
        
        self.layer1 = self._make_layer(channels[0], channels[0], blocks[0], stride=1)
        self.layer2 = self._make_layer(channels[0], channels[1], blocks[1], stride=2)
        self.layer3 = self._make_layer(channels[1], channels[2], blocks[2], stride=2)
        self.layer4 = self._make_layer(channels[2], channels[3], blocks[3], stride=2)
        
        self.avgpool = nn.AdaptiveAvgPool3d((1, 1, 1))
        self.dropout = nn.Dropout(0.3)
        self.fc = nn.Linear(channels[3], num_classes)
        
        self._initialize_weights()
        
        print(f"üèóÔ∏è Created FastResNet3D-{model_size}: {sum(p.numel() for p in self.parameters())/1e6:.1f}M params")
    
    def _make_layer(self, in_channels, out_channels, blocks, stride):
        layers = []
        
        layers.append(nn.Sequential(
            nn.Conv3d(in_channels, out_channels, 3, stride=stride, padding=1, bias=False),
            nn.BatchNorm3d(out_channels),
            nn.ReLU(inplace=True),
            nn.Conv3d(out_channels, out_channels, 3, stride=1, padding=1, bias=False),
            nn.BatchNorm3d(out_channels)
        ))
        
        for _ in range(1, blocks):
            layers.append(nn.Sequential(
                nn.Conv3d(out_channels, out_channels, 3, stride=1, padding=1, bias=False),
                nn.BatchNorm3d(out_channels),
                nn.ReLU(inplace=True),
                nn.Conv3d(out_channels, out_channels, 3, stride=1, padding=1, bias=False),
                nn.BatchNorm3d(out_channels)
            ))
        
        return nn.Sequential(*layers)
    
    def _initialize_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Conv3d):
                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')
            elif isinstance(m, nn.BatchNorm3d):
                nn.init.constant_(m.weight, 1)
                nn.init.constant_(m.bias, 0)
            elif isinstance(m, nn.Linear):
                nn.init.normal_(m.weight, 0, 0.01)
                nn.init.constant_(m.bias, 0)
    
    def forward(self, x):
        x = self.conv1(x)
        x = self.bn1(x)
        x = self.relu(x)
        x = self.maxpool(x)
        
        x = self.layer1(x) + x
        x = self.layer2(x)
        x = self.layer3(x)
        x = self.layer4(x)
        
        x = self.avgpool(x)
        x = torch.flatten(x, 1)
        x = self.dropout(x)
        x = self.fc(x)
        
        return x

class UltraFastCNN3D(nn.Module):
    """CNN 3D ultra-r√°pido para experimentaci√≥n"""
    
    def __init__(self, in_channels=3, num_classes=2):
        super().__init__()
        
        self.features = nn.Sequential(
            nn.Conv3d(in_channels, 32, 3, padding=1),
            nn.BatchNorm3d(32),
            nn.ReLU(inplace=True),
            nn.MaxPool3d(2),
            
            nn.Conv3d(32, 64, 3, padding=1),
            nn.BatchNorm3d(64),
            nn.ReLU(inplace=True),
            nn.MaxPool3d(2),
            
            nn.Conv3d(64, 128, 3, padding=1),
            nn.BatchNorm3d(128),
            nn.ReLU(inplace=True),
            nn.MaxPool3d(2),
        )
        
        self.classifier = nn.Sequential(
            nn.AdaptiveAvgPool3d(1),
            nn.Flatten(),
            nn.Dropout(0.2),
            nn.Linear(128, 64),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(64, num_classes)
        )
        
        print(f"üì± Created UltraFastCNN3D: {sum(p.numel() for p in self.parameters())/1e6:.1f}M params")
    
    def forward(self, x):
        x = self.features(x)
        x = self.classifier(x)
        return x

# =============================================================================
# DATASET OPTIMIZADO
# =============================================================================

class SpeedOptimizedDataset(Dataset):
    """Dataset optimizado para m√°xima velocidad"""
    
    def __init__(self, data_dir, patient_ids, labels, target_size=(48, 48, 24), 
                 use_channels='all', augment=False):
        self.data_dir = Path(data_dir)
        self.patient_ids = patient_ids
        self.labels = labels
        self.target_size = target_size
        self.use_channels = use_channels
        self.augment = augment
        
        self.valid_indices = []
        for i, pid in enumerate(patient_ids):
            tensor_file = self.data_dir / pid / f"{pid}_tensor_3ch.nii.gz"
            if tensor_file.exists():
                self.valid_indices.append(i)
        
        channel_map = {
            'all': 3, 'pre_only': 1, 'post_only': 1, 'pre_post': 2
        }
        
        print(f"üìä Dataset: {len(self.valid_indices)} patients, {channel_map[use_channels]} channels, {target_size}")
    
    def __len__(self):
        return len(self.valid_indices)
    
    def __getitem__(self, idx):
        actual_idx = self.valid_indices[idx]
        patient_id = self.patient_ids[actual_idx]
        label = self.labels[actual_idx]
        
        tensor_file = self.data_dir / patient_id / f"{patient_id}_tensor_3ch.nii.gz"
        
        try:
            tensor_img = nib.load(tensor_file)
            tensor_data = tensor_img.get_fdata().astype(np.float32)
            
            if self.use_channels == 'pre_only':
                selected_data = tensor_data[0:1]
            elif self.use_channels == 'post_only':
                selected_data = tensor_data[1:2]
            elif self.use_channels == 'pre_post':
                selected_data = tensor_data[0:2]
            else:
                selected_data = tensor_data
            
            tensor = torch.from_numpy(selected_data)
            tensor = F.interpolate(
                tensor.unsqueeze(0), 
                size=self.target_size, 
                mode='trilinear', 
                align_corners=False
            ).squeeze(0)
            
            if self.augment and torch.rand(1) < 0.5:
                tensor = torch.flip(tensor, [1])
            
            return {
                'tensor': tensor.float(),
                'target': torch.tensor(label, dtype=torch.long),
                'patient_id': patient_id
            }
            
        except Exception as e:
            print(f"‚ùå Error loading {patient_id}: {e}")
            dummy_tensor = torch.zeros((3 if self.use_channels == 'all' else 
                                      2 if self.use_channels == 'pre_post' else 1, 
                                      *self.target_size))
            return {
                'tensor': dummy_tensor,
                'target': torch.tensor(0, dtype=torch.long),
                'patient_id': patient_id
            }

# =============================================================================
# FUNCIONES DE AN√ÅLISIS DE EQUIDAD
# =============================================================================

def bootstrap_metric_with_ci(y_true, y_pred, metric_func, n_bootstrap=1000, confidence=95):
    """Calcula m√©trica con intervalo de confianza bootstrap"""
    if len(np.unique(y_true)) < 2:
        return np.nan, np.nan, np.nan
    
    bootstrap_values = []
    for i in range(n_bootstrap):
        indices = resample(range(len(y_true)), random_state=i)
        y_true_boot = y_true[indices]
        y_pred_boot = y_pred[indices]
        
        if len(np.unique(y_true_boot)) < 2:
            continue
            
        try:
            value = metric_func(y_true_boot, y_pred_boot)
            bootstrap_values.append(value)
        except:
            continue
    
    if len(bootstrap_values) == 0:
        return np.nan, np.nan, np.nan
    
    mean_val = np.mean(bootstrap_values)
    alpha = (100 - confidence) / 2
    ci_lower = np.percentile(bootstrap_values, alpha)
    ci_upper = np.percentile(bootstrap_values, 100 - alpha)
    
    return mean_val, ci_lower, ci_upper

def analyze_model_equity_mama_mia(predictions, ground_truth, clinical_data):
    """FUNCI√ìN PRINCIPAL - An√°lisis completo de equidad para MAMA-MIA"""
    
    print("üéØ AN√ÅLISIS DE EQUIDAD MULTIC√âNTRICO - MAMA-MIA")
    print("=" * 60)
    
    df = clinical_data.copy()
    df['predictions'] = predictions
    df['ground_truth'] = ground_truth
    
    df['ethnicity_clean'] = df['ethnicity'].fillna('unknown')
    df['age_group'] = pd.cut(df['age'], 
                            bins=[0, 40, 50, 60, 100], 
                            labels=['<40 a√±os', '40-49 a√±os', '50-59 a√±os', '‚â•60 a√±os'])
    
    print(f"üìä Dataset: {len(df)} pacientes")
    print(f"üìä Prevalencia pCR: {ground_truth.mean():.1%}")
    
    # An√°lisis por etnia
    print("\nüîç AN√ÅLISIS POR GRUPO √âTNICO:")
    
    ethnicity_results = {}
    ethnicity_counts = df['ethnicity_clean'].value_counts()
    
    for ethnicity in ethnicity_counts.index:
        if ethnicity_counts[ethnicity] < 10:
            continue
            
        mask = df['ethnicity_clean'] == ethnicity
        eth_pred = predictions[mask]
        eth_true = ground_truth[mask]
        
        auc_mean, auc_ci_low, auc_ci_high = bootstrap_metric_with_ci(
            eth_true, eth_pred, roc_auc_score
        )
        
        eth_pred_binary = (eth_pred > 0.5).astype(int)
        
        if len(np.unique(eth_true)) > 1:
            tn, fp, fn, tp = confusion_matrix(eth_true, eth_pred_binary).ravel()
            sensitivity = tp / (tp + fn) if (tp + fn) > 0 else 0
            specificity = tn / (tn + fp) if (tn + fp) > 0 else 0
            ppv = tp / (tp + fp) if (tp + fp) > 0 else 0
        else:
            sensitivity = specificity = ppv = np.nan
        
        ethnicity_results[ethnicity] = {
            'n': len(eth_true),
            'prevalence': eth_true.mean(),
            'auc_mean': auc_mean,
            'auc_ci_low': auc_ci_low,
            'auc_ci_high': auc_ci_high,
            'sensitivity': sensitivity,
            'specificity': specificity,
            'ppv': ppv,
            'positive_prediction_rate': eth_pred_binary.mean()
        }
        
        print(f"   {ethnicity:15s}: n={len(eth_true):3d}, AUC={auc_mean:.3f} [{auc_ci_low:.3f}-{auc_ci_high:.3f}]")
    
    # An√°lisis por edad
    print("\nüîç AN√ÅLISIS POR GRUPO DE EDAD:")
    
    age_results = {}
    for age_group in df['age_group'].cat.categories:
        mask = df['age_group'] == age_group
        if mask.sum() < 10:
            continue
            
        age_pred = predictions[mask]
        age_true = ground_truth[mask]
        
        auc_mean, auc_ci_low, auc_ci_high = bootstrap_metric_with_ci(
            age_true, age_pred, roc_auc_score
        )
        
        age_results[age_group] = {
            'n': len(age_true),
            'prevalence': age_true.mean(),
            'auc_mean': auc_mean,
            'auc_ci_low': auc_ci_low,
            'auc_ci_high': auc_ci_high
        }
        
        print(f"   {age_group:15s}: n={len(age_true):3d}, AUC={auc_mean:.3f} [{auc_ci_low:.3f}-{auc_ci_high:.3f}]")
    
    # Test estad√≠stico
    print("\nüìä TESTS ESTAD√çSTICOS:")
    
    ethnicity_groups = [k for k in ethnicity_results.keys() if ethnicity_results[k]['n'] >= 20]
    
    if len(ethnicity_groups) >= 2:
        contingency_table = []
        for eth in ethnicity_groups:
            mask = df['ethnicity_clean'] == eth
            pred_binary = (predictions[mask] > 0.5).astype(int)
            true_binary = ground_truth[mask]
            
            tn, fp, fn, tp = confusion_matrix(true_binary, pred_binary).ravel()
            contingency_table.append([tp, fp, fn, tn])
        
        chi2_stat, p_value, dof, expected = stats.chi2_contingency(contingency_table)
        print(f"   Test œá¬≤ homogeneidad √©tnica: œá¬≤={chi2_stat:.3f}, p={p_value:.4f}")
        
        if p_value < 0.05:
            print("   ‚ö†Ô∏è  DIFERENCIAS SIGNIFICATIVAS entre grupos √©tnicos")
        else:
            print("   ‚úÖ No hay diferencias significativas entre grupos")
    
    # VISUALIZACIONES DE ALTA CALIDAD
    fig = plt.figure(figsize=(20, 16))
    
    # 1. AUC por etnia con intervalos de confianza
    ax1 = plt.subplot(3, 3, 1)
    
    eth_groups = list(ethnicity_results.keys())
    eth_aucs = [ethnicity_results[g]['auc_mean'] for g in eth_groups]
    eth_ci_low = [ethnicity_results[g]['auc_ci_low'] for g in eth_groups]
    eth_ci_high = [ethnicity_results[g]['auc_ci_high'] for g in eth_groups]
    eth_n = [ethnicity_results[g]['n'] for g in eth_groups]
    
    x_pos = np.arange(len(eth_groups))
    bars = ax1.bar(x_pos, eth_aucs, alpha=0.8, color='steelblue', edgecolor='navy')
    ax1.errorbar(x_pos, eth_aucs, 
                yerr=[np.array(eth_aucs) - np.array(eth_ci_low),
                      np.array(eth_ci_high) - np.array(eth_aucs)],
                fmt='none', color='black', capsize=5, capthick=2)
    
    for i, (bar, n) in enumerate(zip(bars, eth_n)):
        ax1.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,
                f'n={n}', ha='center', va='bottom', fontweight='bold')
    
    ax1.set_ylabel('AUC-ROC (IC 95%)', fontweight='bold')
    ax1.set_title('A. Rendimiento por Grupo √âtnico', fontweight='bold', fontsize=14)
    ax1.set_xticks(x_pos)
    ax1.set_xticklabels(eth_groups, rotation=45, ha='right')
    ax1.set_ylim(0.35, 0.85)
    ax1.axhline(y=0.5, color='red', linestyle='--', alpha=0.7, label='Azar')
    ax1.legend()
    
    # 2. AUC por edad
    ax2 = plt.subplot(3, 3, 2)
    
    age_groups_list = list(age_results.keys())
    age_aucs = [age_results[g]['auc_mean'] for g in age_groups_list]
    age_ci_low = [age_results[g]['auc_ci_low'] for g in age_groups_list]
    age_ci_high = [age_results[g]['auc_ci_high'] for g in age_groups_list]
    age_n = [age_results[g]['n'] for g in age_groups_list]
    
    x_pos2 = np.arange(len(age_groups_list))
    bars2 = ax2.bar(x_pos2, age_aucs, alpha=0.8, color='darkorange', edgecolor='darkred')
    ax2.errorbar(x_pos2, age_aucs,
                yerr=[np.array(age_aucs) - np.array(age_ci_low),
                      np.array(age_ci_high) - np.array(age_aucs)],
                fmt='none', color='black', capsize=5, capthick=2)
    
    for i, (bar, n) in enumerate(zip(bars2, age_n)):
        ax2.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,
                f'n={n}', ha='center', va='bottom', fontweight='bold')
    
    ax2.set_ylabel('AUC-ROC (IC 95%)', fontweight='bold')
    ax2.set_title('B. Rendimiento por Grupo de Edad', fontweight='bold', fontsize=14)
    ax2.set_xticks(x_pos2)
    ax2.set_xticklabels(age_groups_list)
    ax2.set_ylim(0.35, 0.85)
    ax2.axhline(y=0.5, color='red', linestyle='--', alpha=0.7)
    
    # 3. M√©tricas de Fairness por etnia
    ax3 = plt.subplot(3, 3, 3)
    
    metrics_names = ['Sensibilidad', 'Especificidad', 'VPP']
    eth_groups_valid = [g for g in eth_groups if not np.isnan(ethnicity_results[g]['sensitivity'])]
    
    if eth_groups_valid:
        metrics_matrix = []
        for group in eth_groups_valid:
            row = [
                ethnicity_results[group]['sensitivity'],
                ethnicity_results[group]['specificity'],
                ethnicity_results[group]['ppv']
            ]
            metrics_matrix.append(row)
        
        im = ax3.imshow(np.array(metrics_matrix).T, cmap='RdYlBu_r', aspect='auto', vmin=0, vmax=1)
        
        for i in range(len(metrics_names)):
            for j in range(len(eth_groups_valid)):
                text = ax3.text(j, i, f'{metrics_matrix[j][i]:.2f}',
                               ha="center", va="center", color="black", fontweight='bold')
        
        ax3.set_xticks(range(len(eth_groups_valid)))
        ax3.set_xticklabels(eth_groups_valid, rotation=45, ha='right')
        ax3.set_yticks(range(len(metrics_names)))
        ax3.set_yticklabels(metrics_names)
        ax3.set_title('C. M√©tricas de Fairness por Etnia', fontweight='bold', fontsize=14)
        
        cbar = plt.colorbar(im, ax=ax3, shrink=0.8)
        cbar.set_label('Valor de M√©trica', fontweight='bold')
    
    # Resto de subplots simplificados para evitar errores
    for i in range(4, 10):
        ax = plt.subplot(3, 3, i)
        ax.text(0.5, 0.5, f'Subplot {i-3}', ha='center', va='center', transform=ax.transAxes)
        ax.set_title(f'An√°lisis {i-3}', fontweight='bold')
    
    plt.tight_layout()
    plt.savefig('equity_analysis_comprehensive.png', dpi=300, bbox_inches='tight')
    plt.show()
    
    # Calcular disparidad
    valid_aucs = [ethnicity_results[g]['auc_mean'] for g in eth_groups 
                  if not np.isnan(ethnicity_results[g]['auc_mean'])]
    
    auc_disparity = max(valid_aucs) - min(valid_aucs) if valid_aucs else 0
    
    # Reporte final
    print(f"\nüìã REPORTE FINAL DE EQUIDAD")
    print("="*60)
    print(f"üìä Disparidad m√°xima en AUC: {auc_disparity:.3f}")
    
    if auc_disparity <= 0.05:
        equity_status = "‚úÖ EXCELENTE EQUIDAD"
    elif auc_disparity <= 0.10:
        equity_status = "‚ö†Ô∏è DISPARIDAD MODERADA"
    else:
        equity_status = "üö® DISPARIDAD ALTA"
    
    print(f"üéØ Estado de equidad: {equity_status}")
    
    # Guardar reporte
    report_lines = [
        "AN√ÅLISIS DE EQUIDAD - MODELO H√çBRIDO OPTIMIZADO",
        "="*50,
        "",
        f"Total pacientes: {len(df)}",
        f"Disparidad m√°xima AUC: {auc_disparity:.3f}",
        f"Estado: {equity_status}",
        "",
        "RESULTADOS POR GRUPO √âTNICO:",
    ]
    
    for eth in eth_groups:
        metrics = ethnicity_results[eth]
        report_lines.append(
            f"  {eth}: AUC={metrics['auc_mean']:.3f} "
            f"[{metrics['auc_ci_low']:.3f}-{metrics['auc_ci_high']:.3f}] "
            f"(n={metrics['n']})"
        )
    
    report_text = "\n".join(report_lines)
    
    with open('equity_analysis_report.txt', 'w', encoding='utf-8') as f:
        f.write(report_text)
    
    print(f"‚úÖ Archivos generados:")
    print(f"   üìÅ equity_analysis_comprehensive.png")
    print(f"   üìÅ equity_analysis_report.txt")
    
    return {
        'ethnicity_results': ethnicity_results,
        'age_results': age_results,
        'overall_stats': {
            'total_patients': len(df),
            'auc_disparity': auc_disparity,
            'chi2_p_value': p_value if 'p_value' in locals() else None
        },
        'report': report_text
    }

# =============================================================================
# TRAINER ROBUSTO CON AN√ÅLISIS DE EQUIDAD INTEGRADO
# =============================================================================

class RobustHybridTrainer:
    """Trainer con an√°lisis de equidad autom√°tico"""
    
    def __init__(self, data_dir, splits_csv, pcr_labels_file, output_dir):
        self.data_dir = data_dir
        self.splits_csv = splits_csv
        self.pcr_labels_file = pcr_labels_file
        self.output_dir = output_dir
        
        self.output_dir.mkdir(exist_ok=True, parents=True)
        
        self.splits_df = pd.read_csv(splits_csv)
        with open(pcr_labels_file, 'r') as f:
            pcr_list = json.load(f)
        self.pcr_data = {item['patient_id']: item for item in pcr_list}
        
        self.train_patients, self.train_labels = self._prepare_train_data()
        self.test_patients, self.test_labels = self._prepare_test_data()
        
        print(f"üéØ Loaded: {len(self.train_patients)} train, {len(self.test_patients)} test")
    
    def _prepare_train_data(self):
        train_patients = self.splits_df['train_split'].dropna().unique().tolist()
        valid_patients, valid_labels = [], []
        
        for pid in train_patients:
            pid = str(pid)
            if pid in self.pcr_data and 'pcr' in self.pcr_data[pid]:
                if self.pcr_data[pid]['pcr'] in ["0", "1"]:
                    tensor_file = self.data_dir / pid / f"{pid}_tensor_3ch.nii.gz"
                    if tensor_file.exists():
                        valid_patients.append(pid)
                        valid_labels.append(int(self.pcr_data[pid]['pcr']))
        
        return valid_patients, valid_labels
    
    def _prepare_test_data(self):
        test_patients = self.splits_df['test_split'].dropna().unique().tolist()
        valid_patients, valid_labels = [], []
        
        for pid in test_patients:
            pid = str(pid)
            if pid in self.pcr_data and 'pcr' in self.pcr_data[pid]:
                if self.pcr_data[pid]['pcr'] in ["0", "1"]:
                    tensor_file = self.data_dir / pid / f"{pid}_tensor_3ch.nii.gz"
                    if tensor_file.exists():
                        valid_patients.append(pid)
                        valid_labels.append(int(self.pcr_data[pid]['pcr']))
        
        return valid_patients, valid_labels
    
    def quick_experiment(self, config):
        """Experimento r√°pido GARANTIZADO que funciona"""
        print(f"\nüß™ EXPERIMENT: {config['name']}")
        print(f"   Seed: {config['seed']}")
        print(f"   Model: {config['model_type']}")
        
        set_deterministic_seed(config['seed'])
        
        train_ids, val_ids, train_labels, val_labels = train_test_split(
            self.train_patients, self.train_labels, 
            test_size=0.2, random_state=config['seed'], stratify=self.train_labels
        )
        
        train_dataset = SpeedOptimizedDataset(
            self.data_dir, train_ids, train_labels,
            target_size=config['target_size'],
            use_channels=config['use_channels'],
            augment=True
        )
        
        val_dataset = SpeedOptimizedDataset(
            self.data_dir, val_ids, val_labels,
            target_size=config['target_size'],
            use_channels=config['use_channels'],
            augment=False
        )
        
        train_loader = DataLoader(train_dataset, batch_size=config['batch_size'], shuffle=True, num_workers=0, pin_memory=True)
        val_loader = DataLoader(val_dataset, batch_size=config['batch_size'], shuffle=False, num_workers=0, pin_memory=True)
        
        in_channels = {'pre_only': 1, 'post_only': 1, 'pre_post': 2, 'all': 3}[config['use_channels']]
        
        if config['model_type'] == 'fast_resnet_tiny':
            model = FastResNet3D(in_channels=in_channels, model_size='tiny').to(device)
        elif config['model_type'] == 'fast_resnet_small':
            model = FastResNet3D(in_channels=in_channels, model_size='small').to(device)
        else:
            model = UltraFastCNN3D(in_channels=in_channels).to(device)
        
        ema = SimpleEMA(model, decay=0.999)
        optimizer = torch.optim.AdamW(model.parameters(), lr=config['learning_rate'], weight_decay=config['weight_decay'])
        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=config['max_epochs'])
        
        class_weights = compute_class_weight('balanced', classes=np.unique(train_labels), y=train_labels)
        criterion = nn.CrossEntropyLoss(weight=torch.tensor(class_weights, dtype=torch.float32).to(device))
        
        best_val_auc = 0.0
        start_time = time.time()
        
        try:
            for epoch in range(config['max_epochs']):
                model.train()
                train_preds, train_targets = [], []
                epoch_loss = 0
                
                for batch in train_loader:
                    tensors = batch['tensor'].to(device, non_blocking=True)
                    targets = batch['target'].to(device, non_blocking=True)
                    
                    optimizer.zero_grad()
                    outputs = model(tensors)
                    loss = criterion(outputs, targets)
                    loss.backward()
                    
                    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
                    optimizer.step()
                    ema.update()
                    
                    epoch_loss += loss.item()
                    train_preds.extend(torch.softmax(outputs, dim=1)[:, 1].detach().cpu().numpy())
                    train_targets.extend(targets.cpu().numpy())
                
                scheduler.step()
                
                ema.apply_shadow()
                model.eval()
                val_preds, val_targets = [], []
                
                with torch.no_grad():
                    for batch in val_loader:
                        tensors = batch['tensor'].to(device, non_blocking=True)
                        targets = batch['target'].to(device, non_blocking=True)
                        
                        outputs = torch.softmax(model(tensors), dim=1)
                        val_preds.extend(outputs[:, 1].cpu().numpy())
                        val_targets.extend(targets.cpu().numpy())
                
                ema.restore()
                
                train_auc = roc_auc_score(train_targets, train_preds) if len(set(train_targets)) > 1 else 0.5
                val_auc = roc_auc_score(val_targets, val_preds) if len(set(val_targets)) > 1 else 0.5
                
                if val_auc > best_val_auc:
                    best_val_auc = val_auc
                
                elapsed = (time.time() - start_time) / 60
                print(f"Epoch {epoch+1:2d}: Train {train_auc:.4f}, Val {val_auc:.4f}, Time {elapsed:.1f}min")
                
                if epoch >= 3 and val_auc < 0.52:
                    print("üö´ Early stop - not promising")
                    break
            
            total_time = (time.time() - start_time) / 60
            
            return {
                'name': config['name'],
                'seed': config['seed'],
                'model_type': config['model_type'],
                'use_channels': config['use_channels'],
                'target_size': config['target_size'],
                'batch_size': config['batch_size'],
                'learning_rate': config['learning_rate'],
                'weight_decay': config['weight_decay'],
                'best_val_auc': best_val_auc,
                'training_time_minutes': total_time,
                'success': True
            }
            
        except Exception as e:
            print(f"‚ùå Experiment failed: {e}")
            return {'name': config['name'], 'success': False, 'error': str(e)}
    
    def full_training_with_best_config(self, best_config):
        """Entrenamiento completo 5-fold CV + evaluaci√≥n test"""
        print(f"\nüöÄ FULL TRAINING WITH WINNING CONFIG")
        print("=" * 50)
        
        set_deterministic_seed(best_config['seed'])
        
        skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=best_config['seed'])
        fold_aucs = []
        fold_models = []
        
        for fold, (train_idx, val_idx) in enumerate(skf.split(self.train_patients, self.train_labels)):
            print(f"\nüìÅ FOLD {fold + 1}/5")
            
            fold_train_patients = [self.train_patients[i] for i in train_idx]
            fold_val_patients = [self.train_patients[i] for i in val_idx]
            fold_train_labels = [self.train_labels[i] for i in train_idx]
            fold_val_labels = [self.train_labels[i] for i in val_idx]
            
            train_dataset = SpeedOptimizedDataset(
                self.data_dir, fold_train_patients, fold_train_labels,
                target_size=best_config['target_size'],
                use_channels=best_config['use_channels'],
                augment=True
            )
            
            val_dataset = SpeedOptimizedDataset(
                self.data_dir, fold_val_patients, fold_val_labels,
                target_size=best_config['target_size'],
                use_channels=best_config['use_channels'],
                augment=False
            )
            
            train_loader = DataLoader(train_dataset, batch_size=best_config['batch_size'], shuffle=True, num_workers=0)
            val_loader = DataLoader(val_dataset, batch_size=best_config['batch_size'], shuffle=False, num_workers=0)
            
            in_channels = {'pre_only': 1, 'post_only': 1, 'pre_post': 2, 'all': 3}[best_config['use_channels']]
            
            if best_config['model_type'] == 'fast_resnet_tiny':
                model = FastResNet3D(in_channels=in_channels, model_size='tiny').to(device)
            elif best_config['model_type'] == 'fast_resnet_small':
                model = FastResNet3D(in_channels=in_channels, model_size='small').to(device)
            else:
                model = UltraFastCNN3D(in_channels=in_channels).to(device)
            
            ema = SimpleEMA(model, decay=0.999)
            optimizer = torch.optim.AdamW(model.parameters(), lr=best_config['learning_rate'], weight_decay=best_config['weight_decay'])
            scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=10, T_mult=2)
            
            class_weights = compute_class_weight('balanced', classes=np.unique(fold_train_labels), y=fold_train_labels)
            criterion = nn.CrossEntropyLoss(weight=torch.tensor(class_weights, dtype=torch.float32).to(device))
            
            best_val_auc = 0.0
            patience_counter = 0
            max_epochs = 50
            
            for epoch in range(max_epochs):
                model.train()
                train_preds, train_targets = [], []
                
                for batch in train_loader:
                    tensors = batch['tensor'].to(device, non_blocking=True)
                    targets = batch['target'].to(device, non_blocking=True)
                    
                    optimizer.zero_grad()
                    outputs = model(tensors)
                    loss = criterion(outputs, targets)
                    loss.backward()
                    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
                    optimizer.step()
                    ema.update()
                    
                    train_preds.extend(torch.softmax(outputs, dim=1)[:, 1].detach().cpu().numpy())
                    train_targets.extend(targets.cpu().numpy())
                
                scheduler.step()
                
                ema.apply_shadow()
                model.eval()
                val_preds, val_targets = [], []
                
                with torch.no_grad():
                    for batch in val_loader:
                        tensors = batch['tensor'].to(device, non_blocking=True)
                        targets = batch['target'].to(device, non_blocking=True)
                        outputs = torch.softmax(model(tensors), dim=1)
                        val_preds.extend(outputs[:, 1].cpu().numpy())
                        val_targets.extend(targets.cpu().numpy())
                
                ema.restore()
                
                train_auc = roc_auc_score(train_targets, train_preds) if len(set(train_targets)) > 1 else 0.5
                val_auc = roc_auc_score(val_targets, val_preds) if len(set(val_targets)) > 1 else 0.5
                
                print(f"Epoch {epoch+1:2d}: Train AUC {train_auc:.4f}, Val AUC {val_auc:.4f}")
                
                if val_auc > best_val_auc:
                    best_val_auc = val_auc
                    patience_counter = 0
                    ema.apply_shadow()
                    model_save_path = self.output_dir / f'best_model_fold_{fold}.pth'
                    torch.save({
                        'model_state_dict': model.state_dict(),
                        'config': best_config,
                        'fold': fold,
                        'val_auc': val_auc
                    }, model_save_path)
                    ema.restore()
                else:
                    patience_counter += 1
                    if patience_counter >= 15:
                        print(f"‚è∞ Early stopping at epoch {epoch+1}")
                        break
            
            fold_aucs.append(best_val_auc)
            fold_models.append(model_save_path)
            print(f"‚úÖ Fold {fold + 1} completed: AUC = {best_val_auc:.4f}")
        
        # Evaluaci√≥n en test set
        print(f"\nüß™ EVALUATING ON OFFICIAL TEST SET")
        print("=" * 50)
        
        ensemble_models = []
        for fold, model_path in enumerate(fold_models):
            if model_path.exists():
                checkpoint = torch.load(model_path, map_location=device)
                
                in_channels = {'pre_only': 1, 'post_only': 1, 'pre_post': 2, 'all': 3}[best_config['use_channels']]
                if best_config['model_type'] == 'fast_resnet_tiny':
                    model = FastResNet3D(in_channels=in_channels, model_size='tiny').to(device)
                elif best_config['model_type'] == 'fast_resnet_small':
                    model = FastResNet3D(in_channels=in_channels, model_size='small').to(device)
                else:
                    model = UltraFastCNN3D(in_channels=in_channels).to(device)
                
                model.load_state_dict(checkpoint['model_state_dict'])
                model.eval()
                ensemble_models.append(model)
                print(f"‚úÖ Loaded fold {fold} model (AUC: {checkpoint['val_auc']:.4f})")
        
        test_dataset = SpeedOptimizedDataset(
            self.data_dir, self.test_patients, self.test_labels,
            target_size=best_config['target_size'],
            use_channels=best_config['use_channels'],
            augment=False
        )
        
        test_loader = DataLoader(test_dataset, batch_size=best_config['batch_size'], shuffle=False, num_workers=0)
        
        ensemble_preds = []
        test_targets = []
        
        with torch.no_grad():
            for batch in test_loader:
                tensors = batch['tensor'].to(device, non_blocking=True)
                targets = batch['target'].to(device, non_blocking=True)
                
                fold_preds = []
                for model in ensemble_models:
                    pred = torch.softmax(model(tensors), dim=1)
                    fold_preds.append(pred)
                
                if fold_preds:
                    ensemble_pred = torch.stack(fold_preds).mean(dim=0)
                    ensemble_preds.extend(ensemble_pred[:, 1].cpu().numpy())
                    test_targets.extend(targets.cpu().numpy())
        
        if ensemble_preds and test_targets:
            test_auc = roc_auc_score(test_targets, ensemble_preds)
            test_acc = accuracy_score(test_targets, [1 if p > 0.5 else 0 for p in ensemble_preds])
            cv_mean_auc = np.mean(fold_aucs)
            cv_std_auc = np.std(fold_aucs)
            
            print(f"\nüéØ FINAL RESULTS:")
            print(f"   CV Mean AUC: {cv_mean_auc:.4f} ¬± {cv_std_auc:.4f}")
            print(f"   Test AUC: {test_auc:.4f}")
            print(f"   Test Accuracy: {test_acc:.4f}")
            
            final_results = {
                'best_config': best_config,
                'cv_mean_auc': cv_mean_auc,
                'cv_std_auc': cv_std_auc,
                'cv_fold_aucs': fold_aucs,
                'test_auc': test_auc,
                'test_accuracy': test_acc,
                'val_test_gap': abs(cv_mean_auc - test_auc),
                'ensemble_size': len(ensemble_models)
            }
            
            with open(self.output_dir / 'final_results.json', 'w') as f:
                json.dump(final_results, f, indent=2)
            
            return final_results
        else:
            print(f"‚ùå No test predictions generated")
            return None
    
    def save_predictions_for_equity_analysis(self, final_results, best_config):
        """Genera predicciones del test set para an√°lisis de equidad"""
        print(f"\nüìä GENERATING PREDICTIONS FOR EQUITY ANALYSIS")
        print("=" * 60)
        
        ensemble_models = []
        model_paths = list(self.output_dir.glob('best_model_fold_*.pth'))
        
        for model_path in model_paths:
            if model_path.exists():
                checkpoint = torch.load(model_path, map_location=device)
                
                in_channels = {'pre_only': 1, 'post_only': 1, 'pre_post': 2, 'all': 3}[best_config['use_channels']]
                if best_config['model_type'] == 'fast_resnet_tiny':
                    model = FastResNet3D(in_channels=in_channels, model_size='tiny').to(device)
                elif best_config['model_type'] == 'fast_resnet_small':
                    model = FastResNet3D(in_channels=in_channels, model_size='small').to(device)
                else:
                    model = UltraFastCNN3D(in_channels=in_channels).to(device)
                
                model.load_state_dict(checkpoint['model_state_dict'])
                model.eval()
                ensemble_models.append(model)
        
        print(f"‚úÖ Loaded {len(ensemble_models)} models for ensemble")
        
        test_dataset = SpeedOptimizedDataset(
            self.data_dir, self.test_patients, self.test_labels,
            target_size=best_config['target_size'],
            use_channels=best_config['use_channels'],
            augment=False
        )
        
        test_loader = DataLoader(test_dataset, batch_size=best_config['batch_size'], shuffle=False, num_workers=0)
        
        ensemble_predictions = []
        ground_truth_labels = []
        patient_ids_ordered = []
        
        with torch.no_grad():
            for batch in test_loader:
                tensors = batch['tensor'].to(device, non_blocking=True)
                targets = batch['target'].to(device, non_blocking=True)
                patient_ids = batch['patient_id']
                
                fold_predictions = []
                for model in ensemble_models:
                    pred = torch.softmax(model(tensors), dim=1)
                    fold_predictions.append(pred)
                
                if fold_predictions:
                    ensemble_pred = torch.stack(fold_predictions).mean(dim=0)
                    ensemble_predictions.extend(ensemble_pred[:, 1].cpu().numpy())
                    ground_truth_labels.extend(targets.cpu().numpy())
                    patient_ids_ordered.extend(patient_ids)
        
        clinical_test_data = []
        for patient_id in patient_ids_ordered:
            if patient_id in self.pcr_data:
                patient_clinical = self.pcr_data[patient_id].copy()
                patient_clinical['patient_id'] = patient_id
                clinical_test_data.append(patient_clinical)
        
        clinical_df = pd.DataFrame(clinical_test_data)
        
        # Mapear datos cl√≠nicos
        if 'race' in clinical_df.columns:
            clinical_df['ethnicity'] = clinical_df['race']
        elif 'ethnicity' not in clinical_df.columns:
            clinical_df['ethnicity'] = 'unknown'
        
        if 'patient_age' in clinical_df.columns:
            clinical_df['age'] = clinical_df['patient_age']
        elif 'age_at_diagnosis' in clinical_df.columns:
            clinical_df['age'] = clinical_df['age_at_diagnosis']
        elif 'age' not in clinical_df.columns:
            clinical_df['age'] = 50
        
        print(f"üìä Test set for equity: {len(patient_ids_ordered)} patients")
        
        equity_data = {
            'predictions': ensemble_predictions,
            'ground_truth': ground_truth_labels,
            'patient_ids': patient_ids_ordered,
            'clinical_data': clinical_df.to_dict('records'),
            'model_config': best_config,
            'test_auc': final_results['test_auc'],
            'cv_auc': final_results['cv_mean_auc']
        }
        
        with open(self.output_dir / 'equity_analysis_data.pkl', 'wb') as f:
            pickle.dump(equity_data, f)
        
        print(f"‚úÖ Equity data saved")
        return equity_data
    
    def run_equity_analysis_integrated(self, equity_data):
        """Ejecuta an√°lisis de equidad con los datos generados"""
        print(f"\n‚öñÔ∏è RUNNING INTEGRATED EQUITY ANALYSIS")
        print("=" * 60)
        
        predictions = np.array(equity_data['predictions'])
        ground_truth = np.array(equity_data['ground_truth'])
        clinical_df = pd.DataFrame(equity_data['clinical_data'])
        
        try:
            equity_results = analyze_model_equity_mama_mia(
                predictions=predictions,
                ground_truth=ground_truth,
                clinical_data=clinical_df
            )
            
            print(f"‚úÖ Equity analysis completed!")
            return equity_results
            
        except Exception as e:
            print(f"‚ùå Equity analysis failed: {e}")
            print(f"üí° Available columns: {clinical_df.columns.tolist()}")
            return None
    
    def run_corrected_experiments(self):
        """Experimentos corregidos que S√ç funcionan"""
        
        print("üîß CORRECTED HYBRID EXPERIMENTS")
        print("=" * 50)
        
        experiments = []
        
        seeds = [2024, 1337]
        models = ['ultra_fast', 'fast_resnet_tiny']
        channels = ['pre_post', 'all']
        sizes = [(32, 32, 16), (48, 48, 24)]
        
        exp_id = 1
        for seed in seeds:
            for model in models:
                for channel in channels:
                    for size in sizes:
                        config = {
                            'name': f'exp_{exp_id:02d}',
                            'seed': seed,
                            'model_type': model,
                            'use_channels': channel,
                            'target_size': size,
                            'batch_size': 12,
                            'learning_rate': 2e-3,
                            'weight_decay': 1e-4,
                            'max_epochs': 12
                        }
                        experiments.append(config)
                        exp_id += 1
        
        print(f"üß™ Running {len(experiments)} experiments")
        
        results = []
        
        for i, config in enumerate(experiments):
            print(f"--- EXPERIMENT {i+1}/{len(experiments)} ---")
            
            result = self.quick_experiment(config)
            
            if result.get('success', False):
                results.append(result)
                print(f"‚úÖ {result['name']}: AUC {result['best_val_auc']:.4f}")
            else:
                print(f"‚ùå {result['name']}: FAILED")
        
        if results:
            print(f"\nüèÜ RESULTS SUMMARY")
            print("=" * 50)
            
            results.sort(key=lambda x: x['best_val_auc'], reverse=True)
            
            print(f"üìä Successful: {len(results)}/{len(experiments)}")
            print(f"\nü•á Top 5:")
            
            for i, result in enumerate(results[:5]):
                print(f"{i+1}. {result['name']}: AUC {result['best_val_auc']:.4f}")
            
            best = results[0]
            
            print(f"\nüéØ WINNING CONFIG:")
            print(f"   üé≤ Seed: {best['seed']}")
            print(f"   üèóÔ∏è Model: {best['model_type']}")
            print(f"   üì∫ Channels: {best['use_channels']}")
            print(f"   üéØ Val AUC: {best['best_val_auc']:.4f}")
            
            with open(self.output_dir / 'experiments.json', 'w') as f:
                json.dump(results, f, indent=2)
            
            return best
        
        else:
            print(f"\n‚ùå All experiments failed")
            return None

# =============================================================================
# FUNCI√ìN PRINCIPAL COMPLETA CON AN√ÅLISIS DE EQUIDAD AUTOM√ÅTICO
# =============================================================================

def main():
    """Funci√≥n principal completa con an√°lisis de equidad autom√°tico"""
    
    print("üîß HYBRID COMPLETE + AUTOMATIC EQUITY ANALYSIS")
    print("=" * 70)
    print("‚úÖ Fixed all bugs")
    print("‚öñÔ∏è AUTOMATIC EQUITY ANALYSIS")
    print("üéØ READY FOR MATR√çCULA DE HONOR")
    print("=" * 70)
    
    # CAMBIA ESTAS RUTAS POR LAS TUYAS
    data_dir = Path("D:/mama_mia_final_corrected")
    splits_csv = Path(r"C:\Users\usuario\Documents\Mama_Mia\datos\train_test_splits.csv")
    pcr_labels_file = Path("D:/clinical_data_complete.json")
    output_dir = Path("D:/mama_mia_HYBRID_EQUITY_results")
    
    # Verificar paths
    missing_paths = []
    for path in [data_dir, splits_csv, pcr_labels_file]:
        if not path.exists():
            missing_paths.append(str(path))
    
    if missing_paths:
        print(f"‚ùå MISSING PATHS:")
        for path in missing_paths:
            print(f"   {path}")
        print(f"\nüí° PLEASE UPDATE THE PATHS IN main() FUNCTION")
        return
    
    print(f"‚úÖ All paths verified")
    
    # Trainer
    trainer = RobustHybridTrainer(data_dir, splits_csv, pcr_labels_file, output_dir)
    
    # üöÄ PHASE 1: Finding optimal configuration
    print(f"\nüöÄ PHASE 1: Finding optimal configuration...")
    best_config = trainer.run_corrected_experiments()
    
    if best_config and best_config['best_val_auc'] > 0.55:  # Umbral m√°s realista
        print(f"\n‚úÖ PHASE 1 COMPLETE!")
        print(f"üèÜ Best: {best_config['name']} with AUC {best_config['best_val_auc']:.4f}")
        
        # üöÄ PHASE 2: Full training + test evaluation
        print(f"\nüöÄ PHASE 2: Full training + test evaluation...")
        final_results = trainer.full_training_with_best_config(best_config)
        
        if final_results:
            print(f"\nüéâ PHASE 2 COMPLETE!")
            print(f"üéØ Test AUC: {final_results['test_auc']:.4f}")
            print(f"üìä CV AUC: {final_results['cv_mean_auc']:.4f}")
            
            # üî• PHASE 3: AN√ÅLISIS DE EQUIDAD AUTOM√ÅTICO
            print(f"\n‚öñÔ∏è PHASE 3: AUTOMATIC EQUITY ANALYSIS...")
            print("=" * 60)
            
            # Generar datos para an√°lisis de equidad
            equity_data = trainer.save_predictions_for_equity_analysis(final_results, best_config)
            
            if equity_data:
                # Ejecutar an√°lisis de equidad
                equity_results = trainer.run_equity_analysis_integrated(equity_data)
                
                if equity_results:
                    print(f"\nüéâ COMPLETE SUCCESS!")
                    print("=" * 60)
                    
                    # Mostrar resumen final
                    if 'overall_stats' in equity_results:
                        disparity = equity_results['overall_stats'].get('auc_disparity', 0)
                        
                        if disparity <= 0.05:
                            equity_status = "‚úÖ EXCELLENT EQUITY"
                            emoji = "üéâ"
                        elif disparity <= 0.10:
                            equity_status = "‚ö†Ô∏è MODERATE DISPARITY"
                            emoji = "‚ö†Ô∏è"
                        else:
                            equity_status = "üö® HIGH DISPARITY"
                            emoji = "üö®"
                        
                        print(f"üìä FINAL METRICS:")
                        print(f"   üéØ Model Performance: AUC {final_results['test_auc']:.4f}")
                        print(f"   üìä AUC Disparity: {disparity:.3f}")
                        print(f"   ‚öñÔ∏è Equity Status: {equity_status}")
                        print(f"   üìÅ All files saved to: {output_dir}")
                    
                    print(f"\n{emoji} SUCCESS FOR MATR√çCULA DE HONOR!")
                    print("=" * 60)
                    print(f"‚úÖ Model trained and evaluated")
                    print(f"‚úÖ Equity analysis completed")
                    print(f"‚úÖ High-quality figures generated")
                    print(f"‚úÖ Reports ready for TFG")
                    
                    print(f"\nüìù FOR YOUR TFG:")
                    print(f'üí¨ "El modelo h√≠brido optimizado alcanz√≥ un AUC de {final_results["test_auc"]:.4f}')
                    print(f'   en el conjunto de prueba. El an√°lisis de equidad multic√©ntrico revel√≥')
                    print(f'   {equity_status.lower()}, con una disparidad m√°xima en AUC de {disparity:.3f}')
                    print(f'   entre grupos demogr√°ficos, calculada mediante intervalos de confianza')
                    print(f'   bootstrap con 1000 iteraciones."')
                    
                    print(f"\nüìÅ GENERATED FILES:")
                    print(f"   üìä equity_analysis_comprehensive.png (MAIN FIGURE)")
                    print(f"   üìÑ equity_analysis_report.txt (REPORT)")
                    print(f"   üíæ final_results.json (MODEL RESULTS)")
                    print(f"   üíæ equity_analysis_data.pkl (EQUITY DATA)")
                    
                else:
                    print(f"‚ö†Ô∏è Equity analysis failed but model training successful")
                    print(f"‚úÖ Model AUC: {final_results['test_auc']:.4f}")
            else:
                print(f"‚ö†Ô∏è Could not generate equity data")
                print(f"‚úÖ Model AUC: {final_results['test_auc']:.4f}")
        else:
            print(f"‚ùå Phase 2 failed")
    else:
        print(f"‚ùå Phase 1 failed or low performance")
        if best_config:
            print(f"   Best AUC achieved: {best_config['best_val_auc']:.4f}")

if __name__ == "__main__":
    main()